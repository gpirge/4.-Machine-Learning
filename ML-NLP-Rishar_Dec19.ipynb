{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\alp\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\alp\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108253 sha256=2fec836908a3360c812b4123dd6856a9a05d3c6d92c06ccb59e898d78cec8160\n",
      "  Stored in directory: c:\\users\\alp\\appdata\\local\\pip\\cache\\wheels\\34\\3d\\14\\f19c01a19c9201cdb6a76b049904d5226912569be919ad1eae\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, smart-open, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.15\n",
      "    Uninstalling Cython-0.29.15:\n",
      "      Successfully uninstalled Cython-0.29.15\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.0.1\n",
      "Collecting spaCy\n",
      "  Downloading spacy-2.3.5-cp37-cp37m-win_amd64.whl (9.5 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from spaCy) (1.18.1)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp37-cp37m-win_amd64.whl (6.5 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alp\\anaconda3\\lib\\site-packages (from spaCy) (45.2.0.post20200210)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-win_amd64.whl (176 kB)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp37-cp37m-win_amd64.whl (888 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-win_amd64.whl (108 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from spaCy) (4.42.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from spaCy) (2.22.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\alp\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spaCy) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\alp\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spaCy) (2.2.0)\n",
      "Installing collected packages: catalogue, wasabi, plac, blis, srsly, cymem, murmurhash, preshed, thinc, spaCy\n",
      "Successfully installed blis-0.7.4 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 spaCy-2.3.5 srsly-1.0.5 thinc-7.4.5 wasabi-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"Oh man, @2 two 3kek this is pretty cool. We will do more such things.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = word_tokenize(sample_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', ',', '@', '2', 'two', '3kek', 'this', 'is', 'pretty', 'cool', '.', 'we', 'will', 'do', 'more', 'such', 'things', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Punctuation and Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in text_tokens if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'two', 'this', 'is', 'pretty', 'cool', 'we', 'will', 'do', 'more', 'such', 'things']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_without_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [t for t in tokens_without_punc if t not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'two', 'pretty', 'cool', 'things']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lems = [lem.lemmatize(t) for t in tokens_without_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'two', 'pretty', 'cool', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'two', 'pretti', 'cool', 'thing']\n"
     ]
    }
   ],
   "source": [
    "sts = [st.stem(t) for t in tokens_without_stopwords]\n",
    "print(sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization: \n",
      " ['oh', 'man', ',', '@', '2', 'two', '3kek', 'this', 'is', 'pretty', 'cool', '.', 'we', 'will', 'do', 'more', 'such', 'things', '.'] \n",
      "\n",
      "removing punctation: \n",
      " ['oh', 'man', 'two', 'this', 'is', 'pretty', 'cool', 'we', 'will', 'do', 'more', 'such', 'things'] \n",
      "\n",
      "removing stopwords: \n",
      " ['oh', 'man', 'two', 'pretty', 'cool', 'things'] \n",
      "\n",
      "stemming \n",
      " ['oh', 'man', 'two', 'pretti', 'cool', 'thing'] \n",
      "\n",
      "lemmatization instead of stemming \n",
      " ['oh', 'man', 'two', 'pretty', 'cool', 'thing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenization:\",  \"\\n\", text_tokens,  \"\\n\")\n",
    "print(\"removing punctation:\",  \"\\n\", tokens_without_punc,  \"\\n\")\n",
    "print(\"removing stopwords:\",  \"\\n\", tokens_without_stopwords, \"\\n\")\n",
    "print(\"stemming\",  \"\\n\", sts, \"\\n\")\n",
    "print(\"lemmatization instead of stemming\",  \"\\n\", lems, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
